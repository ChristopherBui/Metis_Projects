{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import json\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/christopher/Desktop/projects/Metis_Projects/Project_4/data/train-v2.0.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionary of wiki articles and their respective content presented as one long string\n",
    "page = {}\n",
    "\n",
    "# dictionary of questions pertaining to specific wiki page\n",
    "qa = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_content(data):\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        page[str(i)] = dict([(\"title\", None), (\"content\", [])])\n",
    "        qa[str(i)] = dict([(\"questions\", []), (\"answers\",[])])\n",
    "\n",
    "\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        page[str(i)][\"title\"] = data[\"data\"][i][\"title\"]\n",
    "        for j in data[\"data\"][i][\"paragraphs\"]:\n",
    "            page[str(i)][\"content\"].append(j[\"context\"])\n",
    "            for k in range(len(j[\"qas\"])):\n",
    "                qa[str(i)][\"questions\"].append(j[\"qas\"][k][\"question\"])\n",
    "                \n",
    "# save unconcatenated paragraphs for each wiki page\n",
    "    pickle_out = open('unconcat_paragraphs.pickle','wb')\n",
    "    pickle.dump(page, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "# concatenate all paragraphs for a wiki page into 1 long string\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        page[str(i)][\"content\"] = ' '.join(page[str(i)][\"content\"])\n",
    "                                                                          \n",
    "# save content & questions\n",
    "    json.dump(page, open('wiki_pages.json','w'))\n",
    "    json.dump(qa, open('wiki_questions.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_content(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create corpus of all wiki pages content\n",
    "corpus_titles = []\n",
    "corpus = []\n",
    "for i in range(len(page)):\n",
    "    corpus_titles.append(page[str(i)][\"title\"])\n",
    "    corpus.append(page[str(i)][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs: 442 \n",
      " corpus_length:  442 \n",
      " corpus_title_length: 442\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "number_of_docs = len(page)\n",
    "corpus_length = len(corpus)\n",
    "corpus_titles_length = len(corpus_titles)\n",
    "print('number of docs:', number_of_docs, '\\n', 'corpus_length: ', corpus_length, '\\n', 'corpus_title_length:', corpus_titles_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_wiki_indices(arr, n):\n",
    "\n",
    "    ordered_cs = {}\n",
    "# select the inputed question's cosine similarity array\n",
    "    cs_q = arr[-1]\n",
    "    \n",
    "# delete cosine similarity value of 1 (the question itself)\n",
    "    cs_q = np.delete(cs_q,-1)\n",
    "\n",
    "\n",
    "# get indices of top n wikis based on highest cosine similarity score\n",
    "    wiki_indices = np.argpartition(cs_q, -n)[-n:]\n",
    "    wiki_indices_list = list(wiki_indices)\n",
    "    \n",
    "    wiki_values_list = list(cs_q[wiki_indices])\n",
    "    \n",
    "# create dictionary with index of cs value as keys, and actual cs value for values \n",
    "    index_cs_values = dict(zip(wiki_indices_list, wiki_values_list))\n",
    "\n",
    "# created ordered dictionary (highest to lowest cs values)\n",
    "    for key,value in sorted(index_cs_values.items(), key=lambda item:(item[1],item[0]), reverse=True):\n",
    "        ordered_cs[key]=value\n",
    "    \n",
    "    return list(ordered_cs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_wiki(own_question=False):\n",
    "    \n",
    "    X_corpus = corpus\n",
    "    X_corpus_titles = corpus_titles\n",
    "    print(len(X_corpus_titles), len(X_corpus))\n",
    "    \n",
    "# choose a question & add it to the corpus\n",
    "    if own_question==True:\n",
    "        own_question = input('Ask a quesiton: ')\n",
    "        X_corpus.append(own_question)\n",
    "    \n",
    "    else:\n",
    "        wiki_num = random.randint(0,len(qa))\n",
    "        chosen_q = random.choice(qa[str(wiki_num)][\"questions\"])\n",
    "        X_corpus.append(chosen_q)\n",
    "        #X_corpus_titles.append('placeholder')\n",
    "        \n",
    "        print(chosen_q)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(strip_accents='ascii', stop_words='english')\n",
    "    X_tfidf = tfidf.fit_transform(X_corpus).toarray()\n",
    "    \n",
    "# convert array of documents & tfidf values to dataframe\n",
    "    df = pd.DataFrame(X_tfidf, columns=tfidf.get_feature_names())\n",
    "    \n",
    "# compute cosine similarity\n",
    "    cs_array = cosine_similarity(df)\n",
    "    \n",
    "# Find which wiki page corresponds highest to question asked by finding\n",
    "# index of highest value in the question's cosine similarity array.\n",
    "# Index in cs_array is mapped 1 to 1 with the index of article titles in X_corpus_titles\n",
    "    best_wiki = X_corpus_titles[np.argmax(np.delete(cs_array[-1],-1))]\n",
    "    \n",
    "# remove appended question to maintain clean corpus\n",
    "    del X_corpus[-1]\n",
    "    \n",
    "    print('Most Relevant Wiki Article: ', best_wiki)\n",
    "    \n",
    "# get indices of top n wiki articles\n",
    "    top_wikis = top_wiki_indices(cs_array, 5)\n",
    "    \n",
    "    top_wikis_list = []\n",
    "    \n",
    "    for index in top_wikis:\n",
    "        top_wikis_list.append(X_corpus_titles[index])\n",
    "        \n",
    "# get first few paragraphs of top wiki article\n",
    "    top_wiki_index = top_wikis[0]\n",
    "\n",
    "# load unconcatenated paragraphs dictionary\n",
    "    pickle_in = open('unconcat_paragraphs.pickle','rb')\n",
    "    paragraphs = pickle.load(pickle_in)\n",
    "\n",
    "    few_paragraphs = []\n",
    "    \n",
    "    for num in range(3):\n",
    "        few_paragraphs.append(paragraphs[str(top_wiki_index)][\"content\"][num])\n",
    "\n",
    "    \n",
    "# find top n titles\n",
    "# print top 3 paragraphs\n",
    "    \n",
    "# best_wiki\n",
    "    return top_wikis_list, few_paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 442\n",
      "Who does the senior leadership roles in the USAF include? \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Wiki Article:  United_States_Air_Force\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['United_States_Air_Force',\n",
       "  'Party_leaders_of_the_United_States_House_of_Representatives',\n",
       "  'Labour_Party_(UK)',\n",
       "  'Order_of_the_British_Empire',\n",
       "  'Korean_War'],\n",
       " [\"The United States Air Force (USAF) is the aerial warfare service branch of the United States Armed Forces and one of the seven American uniformed services. Initially part of the United States Army, the USAF was formed as a separate branch of the military on 18 September 1947 under the National Security Act of 1947. It is the most recent branch of the U.S. military to be formed, and is the largest and one of the world's most technologically advanced air forces. The USAF articulates its core functions as Nuclear Deterrence Operations, Special Operations, Air Superiority, Global Integrated ISR, Space Superiority, Command and Control, Cyberspace Superiority, Personnel Recovery, Global Precision Attack, Building Partnerships, Rapid Global Mobility and Agile Combat Support.\",\n",
       "  'The U.S. Air Force is a military service organized within the Department of the Air Force, one of the three military departments of the Department of Defense. The Air Force is headed by the civilian Secretary of the Air Force, who reports to the Secretary of Defense, and is appointed by the President with Senate confirmation. The highest-ranking military officer in the Department of the Air Force is the Chief of Staff of the Air Force, who exercises supervision over Air Force units, and serves as a member of the Joint Chiefs of Staff. Air Force combat and mobility forces are assigned, as directed by the Secretary of Defense, to the Combatant Commanders, and neither the Secretary of the Air Force nor the Chief of Staff have operational command authority over them.',\n",
       "  'Recently, the Air Force refined its understanding of the core duties and responsibilities it performs as a Military Service Branch, streamlining what previously were six distinctive capabilities and seventeen operational functions into twelve core functions to be used across the doctrine, organization, training, equipment, leadership, and education, personnel, and facilities spectrum. These core functions express the ways in which the Air Force is particularly and appropriately suited to contribute to national security, but they do not necessarily express every aspect of what the Air Force contributes to the nation. It should be emphasized that the core functions, by themselves, are not doctrinal constructs.'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 442\n",
      "Ask a quesiton: who is yeezy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Relevant Wiki Article:  Kanye_West\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Kanye_West',\n",
       "  'Matter',\n",
       "  'Franco-Prussian_War',\n",
       "  'General_Electric',\n",
       "  'Valencia'],\n",
       " ' ',\n",
       " ['Kanye Omari West (/ˈkɑːnjeɪ/; born June 8, 1977) is an American hip hop recording artist, record producer, rapper, fashion designer, and entrepreneur. He is among the most acclaimed musicians of the 21st century, attracting both praise and controversy for his work and his outspoken public persona.',\n",
       "  \"Raised in Chicago, West briefly attended art school before becoming known as a producer for Roc-A-Fella Records in the early 2000s, producing hit singles for artists such as Jay-Z and Alicia Keys. Intent on pursuing a solo career as a rapper, West released his debut album The College Dropout in 2004 to widespread commercial and critical success, and founded record label GOOD Music. He went on to explore a variety of different musical styles on subsequent albums that included the baroque-inflected Late Registration (2005), the arena-inspired Graduation (2007), and the starkly polarizing 808s & Heartbreak (2008). In 2010, he released his critically acclaimed fifth album, the maximalist My Beautiful Dark Twisted Fantasy, and the following year he collaborated with Jay-Z on the joint LP Watch the Throne (2011). West released his abrasive sixth album, Yeezus, to further critical praise in 2013. Following a series of recording delays and work on non-musical projects, West's seventh album, The Life of Pablo, was released in 2016.\",\n",
       "  'West is one of the best-selling artists of all time, having sold more than 32 million albums and 100 million digital downloads worldwide. He has won a total of 21 Grammy Awards, making him one of the most awarded artists of all time and the most Grammy-awarded artist of his age. Three of his albums rank on Rolling Stone\\'s 2012 \"500 Greatest Albums of All Time\" list; two of his albums feature at first and eighth, respectively, in Pitchfork Media\\'s The 100 Best Albums of 2010–2014. He has also been included in a number of Forbes annual lists. Time named him one of the 100 most influential people in the world in 2005 and 2015.'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_wiki(own_question=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
